

wget_usage.txt
  from www & wget_help.txt
wget_help.txt
  wget --help
man_wget.txt
  man wget




=========================================
wget -r -l inf --no-remove-listing --no-parent -p -k -nc -N --no-use-server-timestamps -c --random-wait --wait=8 --tries=30 --limit-rate=200K --compression=auto -U "Mozilla/5.0 (iPhone; CPU iPhone OS 12_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.1.1 Mobile/15E148 Safari/604.1" https://renenyffenegger.ch/notes/tools/Graphviz/
=========================================
wget -r -l inf --no-remove-listing --no-parent -p -k -nc -N --no-use-server-timestamps -c --random-wait --wait=8 --tries=30 --limit-rate=200K --compression=auto -U "Mozilla/5.0 (iPhone; CPU iPhone OS 12_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.1.1 Mobile/15E148 Safari/604.1"    https://free.nchc.org.tw/osdn//storage/g/t/te/termux-old/game-packages-21/ --reject-regex='[^?]*[?].*|(https://free.nchc.org.tw/osdn//storage/g/t/te/termux-old/game-packages-21/|)(aarch64|i686|x86_64)(/.*)?' --accept-regex='(https://free.nchc.org.tw/osdn//storage/g/t/te/termux-old/game-packages-21/|)(all|arm|dists)(/.*)?'
  拒绝的正则表达式:'[^?]*[?].*'用于避免 网页的2**N个排序(目录文件列表-按时间/大小/...排序)
  同时拒绝 cpu指令集 不匹配的
=========================================

wget下载网站一隅之地时的命令
wget -r -l inf --no-remove-listing --no-parent -p -k -nc -N --no-use-server-timestamps -c --random-wait --wait=8 --tries=30 --limit-rate=200K --compression=auto -U "Mozilla/5.0 (iPhone; CPU iPhone OS 12_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.1.1 Mobile/15E148 Safari/604.1" URL...
  =======其他可选项:
  --force-directories --directory-prefix=PREFIX
  --referer=URL
  -e robots=off
  --timeout=SECONDS
  --backup-converted



=========================================
=========================================
=========================================
=========================================
wget选项节选:自https://linuxreviews.org/Wget:_download_whole_or_parts_of_websites_with_ease
wget -r --no-parent
wget -e robots=off
  The command-line option -e robots=off will tell wget to ignore the robots.txt file.
wget -U Mozilla
wget -U "Mozilla/5.0 (iPhone; CPU iPhone OS 12_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.1.1 Mobile/15E148 Safari/604.1"
wget -U "Mozilla/5.0 (Linux; Android 9; CLT-L29) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Mobile Safari/537.36"
  user-agents

wget --wait=20 --limit-rate=20K
  You should add --wait=20 to pause 20 seconds between retrievals - this ensures that you are not manually added to a blacklist. --limit-rate defaults to bytes, add K to set KB/s.

wget --no-parent
  --no-parent is a very handy option that guarantees wget will not download anything from the folders beneath the folder you want to acquire. Use this to make sure wget does not fetch more than it needs to if just just want to download the files in a folder.



wget选项节选:自 wget --help
-np,  --no-parent
-p
  隐含下载组成页面的多媒体文件
-k
  重命名链接
-nc
  不重复下载同一文件
-c
  继续下载
-N,  --timestamping
  使用时间戳，不重复下载旧文件
  -nc -N --no-use-server-timestamps -c
--wait=SECONDS
  重要！推迟重试下载
--limit-rate=RATE
  重要！限制下载速率
  --random-wait --wait=30 --tries=30 --limit-rate=200K

--force-directories
  force creation of directories
-P,  --directory-prefix=PREFIX
  save files to PREFIX/..
  --force-directories -P www.xxx.cn/

--compression=TYPE
  choose compression, one of auto, gzip and none. (default: none)
  --compression=auto

--referer=URL
  include 'Referer: URL' header in HTTP request
-U,  --user-agent=AGENT
  identify as AGENT instead of Wget/VERSION

--no-remove-listing
  don't remove '.listing' files

-r,  --recursive
-l,  --level=NUMBER
  maximum recursion depth (inf or 0 for infinite)
  -N -r -l inf --no-remove-listing




=====其他

--timeout=SECONDS
-i,  --input-file=FILE
  download URLs found in local or external FILE
-F,  --force-html
  treat input file as HTML
-B,  --base=URL
  resolves HTML input-file links (-i -F) relative to URL


Recursive download:
  -r,  --recursive
  -l,  --level=NUMBER
    maximum recursion depth (inf or 0 for infinite)
  -k,  --convert-links
    make links in downloaded HTML or CSS point to local files
  -K,  --backup-converted
    before converting file X, back up as X.orig
  -m,  --mirror
    shortcut for -N -r -l inf --no-remove-listing
  -p,  --page-requisites
    get all images, etc. needed to display HTML page

Recursive accept/reject:
  -A,  --accept=LIST
    comma-separated list of accepted extensions
  -R,  --reject=LIST
    comma-separated list of rejected extensions
  --accept-regex=REGEX
    regex matching accepted URLs
  --reject-regex=REGEX
    regex matching rejected URLs
  --regex-type=TYPE
    regex type (posix|pcre)
  -D,  --domains=LIST
    comma-separated list of accepted domains
  --exclude-domains=LIST
    comma-separated list of rejected domains
  -H,  --span-hosts
    go to foreign hosts when recursive
  -L,  --relative
    follow relative links only
  -I,  --include-directories=LIST
    list of allowed directories
  --trust-server-names
    use the name specified by the redirection URL's last component
  -X,  --exclude-directories=LIST
    list of excluded directories
  -np, --no-parent
    don't ascend to the parent directory



