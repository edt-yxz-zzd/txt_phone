https://en.wikipedia.org/wiki/Time_complexity

time_complexity(algorithm) = SUM` O(f<ops>(n)) * ops `{ops <- elementary_operations}
    #the asymptotic behavior of the complexity
    where
        n - the input size in units of bits needed to represent the input.
        O - the big O notation
        # ops - unit

the worst-case time complexity
the average-case complexity

an algorithm with time complexity O(n) is a linear time algorithm
an algorithm with time complexity O(n^alpha) for some constant alpha>1 is a polynomial time algorithm

amortized analysis
    In computer science, amortized analysis is a method for analyzing a given algorithm's complexity, or how much of a resource, especially time or memory, it takes to execute. The motivation for amortized analysis is that looking at the worst-case run time per operation, rather than per algorithm, can be too pessimistic.
    Online algorithms commonly use amortized analysis.


* O(1)
    constant time
* O(alpha(n))
    inverse Ackermann time
        amortized time per operation using a disjoint set

    alpha(n) = the_inverse_Ackermann_function n

    https://en.wikipedia.org/wiki/Amortized_analysis
    https://en.wikipedia.org/wiki/Ackermann_function#Inverse
    the Ackermann function
        total computable function
        not primitive recursive function

        the Ackermann-Peter function
        the_Ackermann_Peter_function :: UInt -> UInt -> PInt
        the_Ackermann_Peter_function m n = this m n
            this m n
                | m == 0 = n+1
                | n == 0 = this (m-1) 1
                | otherwise = this (m-1) $ this m $ n-1

        [the_inverse_Ackermann_function $ the_Ackermann_Peter_function n n === n]

* O(log^*(n))
    iterated logarithmic time
        distributed coloring of cycles

    https://en.wikipedia.org/wiki/Iterated_logarithm
    log^*(n) = iterated_logarithm_of math.e x
    iterated_logarithm_of :: Real -> Real -> UInt
    iterated_logarithm_of base = log_star where
        log = logarithm_of base
        log_star x
            | x <= 1 = 0
            | otherwise = 1 + log_star (log x)

* O(log log n)
    log-logarithmic time
        amortized time per operation using a bounded priority queue
* O(log n) # DLOGTIME
    logarithmic time
        binary search
* poly(log n) # poly x = x^O(1)
    polylogarithmic time
* O(n^c) where 0<c<1
    fractional power
        searching in a kd-tree
* O(n)
    linear time
* O(n log^* n)
    "n log star n" time
        Seidel's polygon triangulation algorithm. 
* O(n log n)
    quasilinear time
        fastest possible comparison sort
        fast Fourier transform
* O(n^2)
    quadratic time
        bubble sort
        insertion sort
        direct convolution
* O(n^3)
    cubic time
        naive multiplication of two n*n matrices
        calculating partial correlation
* poly(n) # == 2^O(log n) # P
    polynomial time
        Karmarkar's algorithm for linear programming
        AKS primality test
* 2^poly(log n) # QP
    quasi-polynomial time
        best-known O(log2 n)-approximation algorithm for the directed Steiner tree problem
* O(2^n^epsilon) where epsilon>0 # == 2^o(n) # SUBEXP
    sub-exponential time
        best-known algorithm for integer factorization and graph isomorphism
* 2^O(n) # E
    exponential time (with linear exponent)
        solving the traveling salesman problem using dynamic programming
* 2^poly(n) # EXPTIME
    exponential time
        solving matrix chain multiplication via brute-force search
* O(n!)
    factorial time
        solving the traveling salesman problem via brute-force search
* 2^2^poly(n) # 2-EXPTIME
    double exponential time
        deciding the truth of a given statement in Presburger arithmetic

