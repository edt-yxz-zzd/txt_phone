https://builtvisible.com/download-your-website-with-wget/

wget
    -r http://www.yoursite.com
    --convert-links
        // <==> -k
    --html-extension
        // <==> -E
    --user-agent="Googlebot/2.1 (+http://www.googlebot.com/bot.html)"
        // or: -U Mozilla
    --no-parent
        // -np
    --wait=20
        // or: --random-wait
    --limit-rate=500K
    --mirror
        // <==> -r -N -l inf -nr

    ############ may try
    -p
    --restrict-file-names=windows
    --no-clobber
    --continue
    --tries=NUMBER
    --timeout=SECONDS
    --backup-converted
        // <=> -K
    --include-directories=<LIST>
        // <=> -I<LIST>
    --exclude-directories=<LIST>
        // <=> -X<LIST>
    --domains=<LIST>
        // <=> -D<LIST>
        // e.g. -D xxx.com,yyy.cn
    --exclude-domains=LIST
    -H,  --span-hosts
    -e robots=off
    --content-disposition
        // EXPERIMENTAL
        // renaming problem when redirect


https://www.linuxjournal.com/content/downloading-entire-web-site-wget
$ wget \
     --recursive \
     --no-clobber \
     --page-requisites \
     --html-extension \
     --convert-links \
     --restrict-file-names=windows \
     --domains website.org \
     --no-parent \
         www.website.org/tutorials/html/

--recursive: download the entire Web site.

--domains website.org: don't follow links outside website.org.

--no-parent: don't follow links outside the directory tutorials/html/.

--page-requisites: get all the elements that compose the page (images, CSS and so on).

--html-extension: save files with the .html extension.

--convert-links: convert links so that they work locally, off-line.

--restrict-file-names=windows: modify filenames so that they will work in Windows as well.

--no-clobber: don't overwrite any existing files (used in case the download is interrupted and resumed).







https://www.lifewire.com/uses-of-command-wget-2201085
infinite recursion
    -r -l inf
logging output:
    -o xxx.log
multiple sites
    1)
        inputfile:
            http://www.myfileserver.com/file1.zip
            http://www.myfileserver.com/file2.zip
            http://www.myfileserver.com/file3.zip
        -i inputfile
    2)
        inputfile:
            file1.zip
            file2.zip
            file3.zip
        -B http://www.myfileserver.com/ -i inputfile

retry options
    --continue
    --tries=NUMBER
    --timeout=SECONDS
    --random-wait
    --wait=SECONDS

Protecting Download Limits 
    --quota=NUMBER
        // e.g. -Q 100m
    --limit-rate=RATE

Getting Through Security
    --user=yourusername --password=yourpassword
    // Note on a multi user system if somebody runs the ps command they will be able to see your username and password.

file types:
    -A "*.mp3"
        // download only
    -R "*.exe"
        // not download
Cliget
    There is a Firefox add-on called cliget. You can add this to Firefox in the following way.

    Visit https://addons.mozilla.org/en-US/firefox/addon/cliget/ and click the "add to Firefox" button.

    Click the install button when it appears. You will required to restart Firefox.

    To use cliget visit a page or file you wish to download and right click. A context menu will appear called cliget and there will be options to "copy to wget" and "copy to curl".

    Click the "copy to wget" option and open a terminal window and then right click and paste. The appropriate wget command will be pasted into the window.

    Basically, this saves you having to type the command yourself.
