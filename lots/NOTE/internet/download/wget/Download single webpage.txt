Download a working local copy of a webpage
    wget -E -H -k -K -p --random-wait -e robots=off -U "Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.1.6) Gecko/20070802 SeaMonkey/1.1.4" http://www.example.com
    # -P ./output/ --tries=10
    # without "-r" "-m" "-np"

    ###wget --no-parent --wait=20 --limit-rate=500K --mirror --html-extension --convert-links -r -p -U Mozilla http://neoprogrammics.com  -X/lunar_phase_images

download_webpage.bat
    @echo:help: -P ./output/ --tries=10
    wget -E -H -k -K -p --random-wait -e robots=off -N --dont-remove-listing -U "Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.1.6) Gecko/20070802 SeaMonkey/1.1.4" %*
example:
    download_webpage https://developer.android.google.cn/guide/topics/text/creating-input-method -P ./IME/




https://stackoverflow.com/questions/6348289/download-a-working-local-copy-of-a-webpage
Download a working local copy of a webpage

problem:
    wget -p
        wget -p successfully downloads all of the web page's prerequisites (css, images, js).
        However, when I load the local copy in a web browser, the page is unable to load the prerequisites because the paths to those prerequisites haven't been modified from the version on the web.
    httrack
        httrack seems like a great tool for mirroring entire websites,
    but it's unclear to me how to use it to create a local copy of a single page.

options:
    wget -p -k http://www.example.com/
    wget -E -H -k -K -p http://example.com
    wget --random-wait -r -p -e robots=off -U mozilla http://www.example.com

    -U 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.1.6) Gecko/20070802 SeaMonkey/1.1.4'
    -e robots=off
        If you're finding you're still missing images etc.. then try adding this: -e robots=off ..... wget actually reads and respects robots.txt - this really made it hard for me to figure out why nothing worked!
    -H, --span-hosts
        to get resources from foreign hosts use -H, --span-hosts

    wget --cookies=on --save-cookies cookies.txt --keep-session-cookies --post-data 'user=labnol&password=123' http://example.com/login.php
    wget --cookies=on --load-cookies cookies.txt --keep-session-cookies http://example.com/paywall
        login-page,session,cookies
        Fetch pages that are behind a login page. You need to replace user and password with the actual form fields while the URL should point to the Form Submit (action) page.



-p
    get you all the required elements to view the site correctly (css, images, etc).
-k --convert-links
    after the download is complete, change all links (to include those for CSS & images) to allow you to view the page offline as it appeared online.


