So far we have looked at learning methods in which a single hypothesis, chosen from a
hypothesis space, is used to make predictions. The idea ofensemble learningmethods is ENSEMBLE
LEARNING
to select a collection, or ensemble, of hypotheses from the hypothesis space and combine
their predictions. For example, during cross-validation we might generate twenty different
decision trees, and have them vote on the best classification for a new example.


The most widely used ensemble method is calledboosting. To understand how it works, BOOSTING
we need first to explain the idea of aweighted training set.


One specific algorithm, called ADABOOST, is shown in Figure 18.34. ADABOOSThas a very
important property: if the input learning algorithm Lis a weak learningalgorithm—which
means thatLalways returns a hypothesis with accuracy on the training set that is slightly
better than random guessing (i.e., 50%+for Boolean classification)—then ADABOOSTwill
return a hypothesis that classifies the training data perfectlyfor large enough K.
