We can squeeze more out of the data and still get an accurate estimate using a technique
calledk-fold cross-validation. The idea is that each example serves double duty—as training
K-FOLD
CROSS-VALIDATION
data and test data. First we split the data intokequal subsets. We then performkrounds of
learning; on each round 1/kof the data is held out as a test set and the remaining examples
are used as training data. The average test set score of thekrounds should then be a better
estimate than a single score. Popular values forkare 5 and 10—enough to give an estimate
that is statistically likely to be accurate, at a cost of 5 to 10 times longer computation time.
The extreme isk=n, also known as leave-one-out cross-validation orLOOCV.
LEAVE-ONE-OUT
CROSS-VALIDATION
LOOCV Despite the best efforts of statistical methodologists, users frequently invalidate their
results by inadvertently peekingat the test data. Peeking can happen like this: A learning PEEKING
algorithm has various “knobs” that can be twiddled to tune its behavior—for example, various
different criteria for choosing the next attribute in decision tree learning. The researcher
generates hypotheses for various different settings of the knobs, measures their error rates on
the test set, and reports the error rate of the best hypothesis. Alas, peeking has occurred! The
Section 18.4. Evaluating and Choosing the Best Hypothesis 709
reason is that the hypothesis was selectedon the basis of its test set error rate, so information
about the test set has leaked into the learning algorithm.
Peeking is a consequence of using test-set performance to bothchoosea hypothesis and
evaluateit. Thewaytoavoid thisisto reallyhold the test set out—lock it away until you
are completely done with learning and simply wish to obtain an independent evaluation of
the final hypothesis. (And then, if you don’t like the results ...you have to obtain, and lock
away, a completely new test set if you want to go back and find a better hypothesis.) If the
test set is locked away, but you still want to measure performance on unseen data as a way of
selecting a good hypothesis, then divide the available data (without the test set) into a training
set and avalidation set. The next section shows how to use validation sets to find a good VALIDATION SET
tradeoff between hypothesis complexity and goodness of fit.
