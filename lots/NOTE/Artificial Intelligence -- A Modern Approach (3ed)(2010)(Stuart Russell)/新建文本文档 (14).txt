The next question is, what does a solution to the problem look like? We have seen that
any fixed action sequence won’t solve the problem, because the agent might end up in a state
other than the goal. Therefore, a solution must specify what the agent should do foranystate
that the agent might reach. A solution of this kind is called apolicy. It is traditional to denote POLICY
a policy byπ,andπ(s)is the action recommended by the policy πfor states. If the agent
has a complete policy, then no matter what the outcome of any action, the agent will always
know what to do nextThe next question is, what does a solution to the problem look like? We have seen that
any fixed action sequence won’t solve the problem, because the agent might end up in a state
other than the goal. Therefore, a solution must specify what the agent should do foranystate
that the agent might reach. A solution of this kind is called apolicy. It is traditional to denote POLICY
a policy byπ,andπ(s)is the action recommended by the policy πfor states. If the agent
has a complete policy, then no matter what the outcome of any action, the agent will always
know what to do next..


A
policy represents the agent function explicitly and is therefore a description of a simple reflex
agent, computed from the information used for a utility-based agent.
