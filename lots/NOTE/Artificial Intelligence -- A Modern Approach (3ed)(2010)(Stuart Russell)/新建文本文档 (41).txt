1. Write down an expression for the likelihood of the data as a function of the parameter(s).
2. Write down the derivative of the log likelihood with respect to each parameter.
3. Find the parameter values such that the derivatives are zero.
The trickiest step is usually the last. In our example, it was trivial, but we will see that in
many cases we need to resort to iterative solution algorithms or other numerical optimization
techniques, as described in Chapter 4. The example also illustrates a significant problem
with maximum-likelihood learning in general:when the data set is small enough that some
events have not yet been observed—for instance, no cherry candies—the maximum-likelihood
hypothesis assigns zero probability to those events. Various tricks are used to avoid this
problem, such as initializing the counts for each event to 1 instead of 0.
