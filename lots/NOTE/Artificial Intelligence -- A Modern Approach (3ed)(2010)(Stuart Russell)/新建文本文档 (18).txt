Bellman equation for utilities
value iteration algorithm
Bellman update
the Bellman update is a contraction by a factor ofγon the space of utility vectors.

In practice, it often occurs thatπibecomes optimal long beforeUihas converged.

Now we have everything we need to use value iteration in practice. We know that
it converges to the correct utilities, we can bound the error in the utility estimates if we
stop after a finite number of iterations, and we can bound the policy loss that results from
executing the corresponding MEU policy. As a final note, all of the results in this section
depend on discounting withγ<1.Ifγ=1and the environment contains terminal states,
then a similar set of convergence results and error bounds can be derived whenever certain
technical conditions are satisfied.
