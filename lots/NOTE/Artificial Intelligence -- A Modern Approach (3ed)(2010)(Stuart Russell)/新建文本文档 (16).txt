The first question to answer is whether there is afinite horizonor aninfinite horizon FINITE HORIZON
INFINITE HORIZON for decision making. A finite horizon means that there is afixedtimeNafter which nothing
matters—the game is over, so to speak. Thus,Uh([s0,s1,... ,sN+k])=Uh([s0,s1,... ,sN])
for allk>0. For example, suppose an agent starts at (3,1) in the 4×3world of Figure 17.1,
and suppose thatN=3. Then, to have any chance of reaching the +1 state, the agent must
head directly for it, and the optimal action is to goUp. On the other hand, if N=100,
then there is plenty of time to take the safe route by going Left. So, with a finite horizon,
the optimal action in a given state could change over time. We say that the optimal policy
for a finite horizon is nonstationary. With no fixed time limit, on the other hand, there is
NONSTATIONARY
POLICY
no reason to behave differently in the same state at different times. Hence, the optimal ac-tion depends only on the current state, and the optimal policy is stationary. Policies for the STATIONARY POLICY
infinite-horizon case are therefore simpler than those for the finite-horizon case, and we deal
mainly with the infinite-horizon case in this chapter. (We will see later that for partially ob-servable environments, the infinite-horizon case is not so simple.) Note that “infinite horizon”
does not necessarily mean that all state sequences are infinite; it just means that there is no
fixed deadline. In particular, there can be finite state sequences in an infinite-horizon MDP
containing a terminal state.
