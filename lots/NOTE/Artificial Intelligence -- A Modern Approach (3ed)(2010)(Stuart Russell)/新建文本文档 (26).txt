As we saw earlier, ifHis the set of all Boolean functions on nattributes, then|H| =
2
2
n
. Thus, the sample complexity of the space grows as 2
n
. Because the number of possible
examples is also2
n
, this suggests that PAC-learning in the class of all Boolean functions
requires seeing all, or nearly all, of the possible examples. A momentâ€™s thought reveals the
reason for this: Hcontains enough hypotheses to classify any given set of examples in all
possible ways. In particular, for any set ofNexamples, the set of hypotheses consistent with
those examples contains equal numbers of hypotheses that predict xN+1to be positive and
hypotheses that predictxN+1to be negative.
To obtain real generalization to unseen examples, then, it seems we need to restrict
the hypothesis space Hin some way; but of course, if we do restrict the space, we might
eliminate the true function altogether. There are three ways to escape this dilemma. The first,
which we will cover in Chapter 19, is to bring prior knowledge to bear on the problem. The
second, which we introduced in Section 18.4.3, is to insist that the algorithm return not just
any consistent hypothesis, but preferably a simple one (as is done in decision tree learning). In
cases where finding simple consistent hypotheses is tractable, the sample complexity results
are generally better than for analyses based only on consistency. The third escape, which
we pursue next, is to focus on learnable subsets of the entire hypothesis space of Boolean
functions. This approach relies on the assumption that the restricted language contains a
hypothesishthat is close enough to the true function f; the benefits are that the restricted
hypothesis space allows for effective generalization and is typically easier to search. We now
examine one such restricted language in more detail.
