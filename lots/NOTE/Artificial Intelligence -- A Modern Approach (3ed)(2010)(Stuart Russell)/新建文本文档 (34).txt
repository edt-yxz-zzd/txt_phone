An interesting thing happens as the ensemble sizeKincreases. Figure 18.35(b) shows
the training set performance (on 100 examples) as a function of K. Notice that the error
reaches zero whenKis 20; that is, a weighted-majority combination of 20 decision stumps
suffices to fit the 100 examples exactly. As more stumps are added to the ensemble, the error
remains at zero. The graph also shows that the test set performance continues to increase
long after the training set error has reached zero. AtK=20, the test performance is 0.95
(or 0.05 error), and the performance increases to 0.98 as late asK= 137, before gradually
dropping to 0.95.
This finding, which is quite robust across data sets and hypothesis spaces, came as quite
a surprise when it was first noticed. Ockhamâ€™s razor tells us not to make hypotheses more
complex than necessary, but the graph tells us that the predictionsimprove as the ensemble
hypothesis gets more complex! Various explanations have been proposed for this. One view
is that boosting approximates Bayesian learning(see Chapter 20), which can be shown to
be an optimal learning algorithm, and the approximation improves as more hypotheses are
added. Another possible explanation is that the addition of further hypotheses enables the
ensemble to bemore definitein its distinction between positive and negative examples, which
helps it when it comes to classifying new examples.
