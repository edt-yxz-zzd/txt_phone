The balance of risk and reward changes depending on the value ofR(s)for the nonter-minal states. Figure 17.2(b) shows optimal policies for four different ranges ofR(s).When
R(s)≤−1.6284, life is so painful that the agent heads straight for the nearest exit, even if
the exit is worth –1. When −0.4278≤R(s)≤−0.0850, life is quite unpleasant; the agent
takes the shortest route to the +1 state and is willing to risk falling into the –1 state by acci-dent. In particular, the agent takes the shortcut from (3,1). When life is only slightly dreary
(−0.0221<R(s)<0), the optimal policy takesno risks at all. In (4,1) and (3,2), the agent
heads directly away from the –1 state so that it cannot fall in by accident, even though this
means banging its head against the wall quite a few times. Finally, ifR(s)>0, then life is
positively enjoyable and the agent avoidsbothexits. As long as the actions in (4,1), (3,2),
and (3,3) are as shown, every policy is optimal, and the agent obtains infinite total reward be-cause it never enters a terminal state. Surprisingly, it turns out that there are six other optimal
policies for various ranges ofR(s); and (3,3) are as shown, every policy is optimal, and the agent obtains infinite total reward be-cause it never enters a terminal state. Surprisingly, it turns out that there are six other optimal
policies for various ranges ofR(s); 
