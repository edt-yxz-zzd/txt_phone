Furthermore, under certain conditions (that hold in ost cases), EM can be proven to reach
a local maximum in likelihood. (In rare cases, it could reach a saddle point or even a local
minimum.) In this sense, EM resembles a gradient-based hill-climbing algorithm, but notice
that it has no “step size” parameter.


Things do not always go as well as Figure 20.12(a) might suggest. It can happen, for
example, that one Gaussian component shrinks so that it covers just a single data point. Then
its variance will go to zero and its likelihood will go to infinity! Another problem is that
two components can “merge,” acquiring identical means and variances and sharing their data
points. These kinds of degenerate local maxima are serious problems, especially in high
dimensions. One solution is to place priors on the model parameters and to apply the MAP
version of EM. Another is to restart a component with new random parameters if it gets too
small or too close to another component. Sensible initialization also helps.
