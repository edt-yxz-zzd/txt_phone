You could say that SVMs are successful because of one key insight and one neat trick.



Instead of minimizing expectedempirical losson the training
data, SVMs attempt to minimize expectedgeneralizationloss.


there are some arguments from
computational learning theory (Section 18.5) suggesting that we minimize generalization loss
by choosing the separator that is farthest away from the examples we have seen so far. We
call this separator, shown in Figure 18.30(b) themaximum margin separator.


an alternative representation called the dual representation


Thisisa quadratic programming
QUADRATIC
PROGRAMMING
optimization problem, for which there are good software packages.


There are three important properties of Equation (18.13). First, the expression
is convex; it has a single global maximum that can be found efficiently. Second,the data enter
the expression only in the form of dot products of pairs of points. This second property is also
true of the equation for the separator itself.
A final important property is that the weightsαj associated with each data point arezeroex-cept for thesupport vectors—the points closest to the separator. (They are called “support” SUPPORT VECTOR
vectors because they “hold up” the separating plane.) Because there are usually many fewer
support vectors than examples, SVMs gain some of the advantages of parametric models.



This phenomenon is actually fairly general: if
data are mapped into a space of sufficiently high dimension, then they will almost always be
linearly separable—if you look at a set of points from enough directions, you’ll find a way to
make them line up.
In general (with some
special cases excepted) if we haveNdata points then they will always be separable in spaces
ofN−1dimensions or more (Exercise 18.25).


Now, we would not usually expect to find a linear separator in the input spacex,but
we can find linear separators in the high-dimensional feature spaceF(x)simply by replacing
xj·xkin Equation (18.13) withF(xj)·F(xk). This by itself is not remarkable—replacing xby
F(x)inanylearning algorithm has the required effect—but the dot product has some special
properties. It turns out thatF(xj)·F(xk)can often be computed without first computingF for each point.


In our three-dimensional feature space defined by Equation (18.15), a little bit
of algebra shows that
F(xj)·F(xk)=(xj·xk)
2
.
(That’s why the
√
2is in f3.) The expression (xj · xk)
2
is called a kernel function,
12
and KERNEL FUNCTION
is usually written as K(xj,xk). The kernel function can be applied to pairs of input data to
evaluate dot products in some corresponding feature space.


A venerable result in mathematics, Mercer’s theo-rem(1909), tells us that any “reasonable”
13
kernel function corresponds tosome feature MERCER’S THEOREM
space. These feature spaces can be very large, even for innocuous-looking kernels. For ex-ample, thepolynomial kernel, K(xj,xk)=(1 +xj · xk)
d
, corresponds to a feature space
POLYNOMIAL
KERNEL
whose dimension is exponential ind.


This then is the cleverkernel trick: Plugging these kernels into Equation (18.13), KERNEL TRICK
optimal linear separators can be found efficiently in feature spaces with billions of (or, in
some cases, infinitely many) dimensions.The resulting linear separators, when mapped back
to the original input space, can correspond to arbitrarily wiggly, nonlinear decision bound-aries between the positive and negative examples.
