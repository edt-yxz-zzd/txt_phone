These results are very comforting, and it is easy to see that they can be extended to any
Bayesian network whose conditional probabilities are represented as tables. The most impor-tant point is that, with complete data, the maximum-likelihood parameter learning problem
for a Bayesian network decomposes into separate learning problems, one for each parameter.
(See Exercise 20.6 for the nontabulated case, where each parameter affects several conditional
probabilities.) The second point is that the parameter values for a variable, given its parents,
are just the observed frequencies of the variable values for each setting of the parent values.
As before, we must be careful to avoid zeroes when the data set is small.
