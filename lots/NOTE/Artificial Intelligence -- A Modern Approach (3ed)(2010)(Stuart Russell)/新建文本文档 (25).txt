18.4.2 From error rates to loss
So far, we have been trying to minimize error rate. This is clearly better than maximizing
error rate, but it is not the full story. Consider the problem of classifying email messages
as spam or non-spam. It is worse to classify non-spam as spam (and thus potentially miss
an important message) then to classify spam as non-spam (and thus suffer a few seconds of
annoyance). So a classifier with a 1% error rate, where almost all the errors were classifying
spam as non-spam, would be better than a classifier with only a 0.5% error rate, if most of
those errors were classifying non-spam as spam. We saw in Chapter 16 that decision-makers
should maximize expected utility, and utility is what learners should maximize as well. In
machine learning it is traditional to express utilities by means of aloss function.
