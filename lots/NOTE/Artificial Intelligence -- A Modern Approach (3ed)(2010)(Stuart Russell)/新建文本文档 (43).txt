The point here is different. If we consider just the parametersθ1
andθ2that define the linear relationship between xandy, it becomes clear that maximizing
the log likelihood with respect to these parameters is the same as minimizingthe numerator
(y−(θ1x+θ2))
2
in the exponent of Equation (20.5). This is the L2loss, the squared er-ror between the actual value yand the predictionθ1x+θ2. This is the quantity minimized
by the standardlinear regression procedure described in Section 18.6. Now we can under-stand why: minimizing the sum of squared errors gives the maximum-likelihood straight-line
model,provided that the data are generated with Gaussian noise of fixed variance.
