CHAPTER 10 Testing

Quality Assurance (QA)

TIP
    Writing automated tests is the most important thing you can do to ensure that you don¡¯t break your users¡¯ programs.

unit testing, integration testing, and performance testing

10.1 REASONS TO WRITE TESTS
pros
    ? Increased confidence. 
        Having an extensive suite of automated tests can give you the confidence to make changes to the behavior of an API with the knowledge that you are not breaking functionality. 
            Said differently, testing can reduce your fear of implementing a change. 
                Its quite common to find legacy systems where engineers are uneasy changing certain parts of the code because those parts are so complex and opaque that changes to its behavior could have unforeseen consequences (Feathers, 2004). 
            Furthermore, the code in question may have been written by an engineer who is no longer with the organization, meaning that there is no one who knows where the bodies are buried in the code.
    ? Ensuring backward compatibility. 
        Its important to know that you have not introduced changes in a new version of the API that breaks backward compatibility for code written against an older version of the API or for data files generated by that older API. 
        Automated tests can be used to capture the workflows and behavior from earlier versions so that these are always exercised in the latest version of the library.
    ? Saving costs. 
        It is a well-known fact that fixing defects later in the development cycle is more expensive than fixing them earlier. 
            This is because the defect becomes more deeply embedded in the code and exorcising it may also involve updating many data files. 
                For example, Steve McConnell gives evidence that fixing a bug after release can be 10~25 times more expensive than during development (McConnell, 2004). 
        Developing a suite of automated tests lets you discover defects earlier so that they can be fixed earlier, and hence more economically overall.
    ? Codify uses cases. 
        Use cases for an API represent supported workflows that your users should be able to accomplish. 
            Developing tests for these use cases before you implement your API can let you know when you have achieved the required functionality. 
        These same tests can then be used on an ongoing basis to catch any regressions in these high-level workflows.
    ? Compliance assurance. 
        Software for use in certain safety- or security-critical applications may have to pass regulatory tests, such as Federal Aviation Administration certification. 
        Also, some organizations may verify that your software conforms to their standards before allowing it to be branded as such. 
            For example, the Open Geospatial Consortium (OGC) has a compliance testing program for software that is to be branded as Certified OGC Compliant. 
        Automated tests can be used to ensure that you continue to conform to these regulatory and standards requirements.
    
    
    These points can be summarized by stating that automated testing can help you determine 
        whether you are building the right thing (referred to as validation) 
        and if you are building it right (called verification).
cons
    It is worth noting that there can be a downside to writing many tests. 
    As the size of your test suite grows, the maintenance for these tests also grows commensurately. 
    This can result in situations where a good code change that takes a couple of minutes to make might also break hundreds of tests and require many hours of effort to also update the test suite. 
    This is a bad situation to get into because it disincentivizes an engineer from making a good fix due only to the overhead of updating the tests. 
    I will discuss ways to avoid this situation in the following sections. 
    However, it¡¯s worth noting that if the fix in question changes the public API, then the extra barrier may be a good thing as it forces the engineer to consider the potential impact on backward compatibility for existing clients.

TIP
    API testing should involve a combination of unit and integration testing. 
    Non functional techniques can also be applied as appropriate

non-functional testings:
    ? Performance testing
        Verifies that the functionality of your API meets minimum speed or memory usage requirements.
    ? Load testing == stress testing
        Puts demand, or stress, on a system and measures its ability to handle this load.
        This often refers to testing with many simultaneous users or performing many API requests per second. 
    ? Scalability testing == capacity testing == volume testing
        Ensures that the system can handle large and complex production data inputs instead of only simple test data sets. 
    ? Soak testing
        Attempts to run software continuously over an extended period to satisfy clients that it is robust and can handle sustained use 
            (e.g., that there are no major memory leaks, counter overflows, or timer-related bugs).
    ? Security testing
        Ensures that any security requirements of your code are met, 
            such as the confidentiality, authentication, authorization, integrity, and availability of sensitive information.
    ? Concurrency testing
        Verifies the multithreaded behavior of your code to ensure that it behaves correctly and does not deadlock.


10.2.1 Unit Testing
    A unit test is used to verify a single minimal unit of source code, such as an individual method or class. 
    The purpose of unit testing is to isolate the smallest testable parts of an API and verify that they function correctly in isolation.

    These kinds of tests tend to run very fast and often take the form of a sequence of assertions that return either true or false, where any false result will fail the test. 
    Very often these tests are colocated with the code that they test (such as in a tests subdirectory), and they can be compiled and run at the same point that the code itself is compiled. 
    Unit tests tend to be written by developers using knowledge of the implementation; therefore, unit testing is a white box testing technique.


    in real software the method or object under test often depends on other objects in the system or on external resources such as files on disk, records in a database, or software on a remote server. 
    This leads to two different views of unit testing.
        1. Fixture setup. 
            The classic approach to unit testing is to initialize a consistent environment, or fixture, before each unit test is run. 
                For example, to ensure that dependent objects and singletons are initialized, to copy a specific set of files to a known location, or to load a database with a prepared set of initial data. 
            This is often done in a setUp() function associated with each test to differentiate test setup steps from the actual test operations. 
                A related tearDown() function is often used to clean up the environment once the test finishes. 
            One of the benefits of this approach is that the same fixture can often be reused for many tests.
        2. Stub/mock objects. 
            In this approach, the code under test is isolated from the rest of the system by creating stub or mock objects that stand in for any dependencies outside of the unit (Mackinnon et al., 2001). 
                For example, if a unit test needs to communicate with a database, a stub database object can be created that accepts the subset of queries that the unit will generate and then return canned data in response, without making any actual connection to the database. 
            The result is a completely isolated test that will not be affected by database problems, network issues, or file system permissions. 
                The downside, however, is that the creation of these stub objects can be tedious and often they cannot be reused by other unit tests. 
                However, mock objects tend to be more flexible and can be customized for individual tests. 


TIP
    If your code depends on an unreliable resource, such as a database, file system, or network, consider using stub or mock objects to produce more robust unit tests.

10.2.2 Integration Testing
    In contrast to unit testing, integration testing is concerned with the interaction of several components cooperating together. 
        Ideally, the individual components have already been unit tested.
    Integration tests are still necessary even if you have a high degree of unit test coverage 
        because testing individual units of code in isolation does not guarantee 
            that they can be used together easily and efficiently 
            or that they meet your functional requirements and use cases.
TIP
    Integration testing is a black box technique to verify the interaction of several components together.

integration tests usually involve more complex ways to verify that a sequence of operations was successful. 
    # update the baseline version
    For example, a test may generate an output file that must be compared against a ¡°golden¡± or ¡°baseline¡± version that is stored with the test in your revision control system. 
        This requires an efficient workflow to update the baseline version in cases where the failure is expected, such as the conscious addition of new elements in the data file or changing the behavior of a function to fix a bug.
A good integration testing framework will therefore include dedicated comparison functions (or diff commands) for each file type that the API can produce.
    # tolerance
    For example, an API may have an ASCII configuration file, where an integration test failure should only be triggered 
        if the value or number of settings changes, 
        but not if the order of the settings in the file changes 
            or if different whitespace characters are used to separate settings. 
    As another example, an API may produce an image as its result. 
        You therefore need a way to compare the output image against a baseline version of that image. 
            For example, the R&D team at PDI/Dreamworks developed a perceptual image difference utility to verify that the rendered images for their film assets are visibly the same after a change to their animation system. 
            This perceptually based comparison allows for minor imperceptible differences in the actual pixel values to avoid unnecessary failures (Yee and Newman, 2004).

integration testing may also be data driven. 
    # data driven
    That is, a single test program can be called many times with different input data. 
        For example, a C++ parser may be verified with a single integration test that reads a .cpp source file and outputs its derivation or abstract syntax tree. 
        That test can then be called many times with different C++ source programs and its output compared against a correct baseline version in each case. 

Given that integration tests have a different focus than unit tests, may be maintained by a different team, and normally must be run after the build has completed successfully, these kinds of tests are therefore usually located in a different directory than unit tests. 
    For example, they may live in a sibling directory to the top-level source directory rather than being stored next to the actual code inside the source directory. 
    This strategy also reflects the black box nature of integration tests as compared to white box unit tests.




TIP
    Performance testing of your key use cases helps you avoid introducing speed or memory regressions unknowingly.
The benefit of writing automated performance tests is that you can make sure that new changes do not adversely impact performance. 
    For example, a senior engineer I worked with once refactored a data loading API to use a std::string object instead of a char buffer to store successive characters read from a data file. 
        When the change was released, users found that the system took over 10 times longer to load their data files. 
        It turns out that the std::string::append() method was reallocating the string each time, growing it by a single byte on each call and hence causing massive amounts of memory allocations to happen. 
        This was ultimately fixed by using a std::vector<char> because the append() method for that container behaved more optimally. 
    A performance test that monitored the time to load large data files could have discovered this regression before it was released to clients.











10.3.1 Qualities of a Good Test
    ? Fast. 
        Your suite of tests should run very quickly so that you get rapid feedback on test failures.
            Unit tests should always be very fast: in the order of fractions of a second per test. 
            Integration tests that perform actual user workflows, or data-driven integration tests that are run on many input files, may take longer to execute. 
        However, there are several ways to deal with this, 
            such as favoring the creation of many unit tests but a few targeted integration tests. 
            Also, you can have different categories of tests: 
                fast (or checkin or continuous) tests run during every build cycle, 
                whereas slow (or complete or acceptance) tests run only occasionally, such as before a release.
    ? Stable. 
        Tests should be repeatable, independent, and consistent: every time you run a specific version of a test you should get the same result. 
            If a test starts failing erroneously or erratically then your faith in the validity of that tests results will be diminished. 
            You may even be tempted to turn the test off temporarily, which of course defeats the purpose of having the test. 
        Using mock objects, where all dependencies of a unit test are simulated, is one way to produce tests that are independent and stable to environmental conditions. 
            Its also the only practical way to test date- or time-dependent code.
    ? Portable. 
        If your API is implemented on multiple platforms, your tests should work across the same range of platforms. 
            One of the most common areas of difference for test code running on different platforms is floating point comparisons. 
                Rounding errors, architecture differences, and compiler differences can cause mathematical operations to produce slightly different results on different platforms. 
                    Floating point comparisons should therefore allow for a small error, or epsilon, rather than being compared exactly. 
                It's important to note that this epsilon should be relative to the magnitude of the numbers involved and the precision of the floating-point type used. 
                    For instance, single-precision floats can represent only six to seven digits of precision. 
                    Therefore, an epsilon of 0.000001 may be appropriate when comparing numbers such as 1.234567, but an epsilon of 0.1 would be more appropriate when comparing numbers such as 123456.7.
    ? High coding standards. 
        Test code should follow the same coding standards as the rest of your API: 
            you should not slip standards just because the code will not be run directly by your users.
            Tests should be well documented so that its clear what is being tested and what a failure would imply. 
                If you enforce code reviews for your API code, you should do the same for test code. 
            Similarly, you should not abandon your good engineering instincts simply because you are writing a test. 
            If there is a case for factoring out common test code into a reusable test library, then you should do this. 
                As the size of your test suite grows, you could end up with hundreds or thousands of tests. 
        The need for robust and maintainable test code is therefore just as imperative as for your main API code.
    ? Reproducible failure. 
        If a test fails, it should be easy to reproduce the failure. 
            This means logging as much information as possible about the failure, pinpointing the actual point of failure as accurately as possible, and making it easy for a developer to run the failing test in a debugger.
        Some systems employ randomized testing (called ad hoc testing) where the test space is so large that random samples are chosen. 
            In these cases, you should ensure that it is easy to reproduce the specific conditions that caused the failure because simply rerunning the test will pick another random sample and may pass.


10.3.2 What to Test
    ? Condition testing. 
        When writing unit tests, you should use your knowledge of the code under test to exercise all combinations of any if/else, for, while, and switch expressions within the unit. 
            This ensures that all possible paths through the code have been tested. 
                statement coverage
                decision coverage
    ? Equivalence classes. 
        An equivalence class is a set of test inputs that all have the same expected behavior. 
        The technique of equivalence class partitioning therefore attempts to find test inputs that exercise difference classes of behavior. 
            For example, consider a square root function that is documented to accept values in the range 0 to 65535. In this case there are three equivalence classes: 
                negative numbers, the valid range of numbers, and numbers greater than 65535. 
                You should therefore test this function with values from each of these three equivalence classes, for example, -10, 100, 100000.
    ? Boundary conditions. 
        Most errors occur around the boundary of expected values. 
            How many times have you inadvertently written code with an off-by-one error? 
        Boundary condition analysis focuses test cases around these boundary values. 
            For example, if you are testing a routine that inserts an element into a linked list of length n, you should test inserting at position 0, 1, n-1, and n.
    ? Parameter testing. 
        A test for a given API call should vary all parameters to the function to verify the full range of functionality. 
            For example, the stdio.h function fopen() accepts a second argument to specify the file mode. 
                This can take the values r, w, and a, in addition to optional + and b characters in each case. 
                A thorough test for this function should therefore test all 12 combinations of the mode parameter to verify the full breadth of behavior.
    ? Return value assertion. 
        This form of testing ensures that a function returns correct results for different combinations of its input parameters. 
        These results could be the return value of the function, but they could additionally include output parameters that are passed as pointers or references. 
            For instance, a simple integer multiplication function, int Multiply(int x, int y) could be tested by supplying a range of (x, y) inputs and checking the results against a table of known correct values.
    ? Getter/setter pairs. 
        The use of getter/setter methods is extremely common in C++ APIs, and of course Ive advocated that you should always prefer the use of these functions over directly exposing member variables in a class. 
        You should therefore test that calling the getter before calling the setter returns an appropriate default result, and that calling the getter after the setter will return the appropriate value, 
            for example,
                AssertEqual(obj.GetValue(), 0, "test default");
                obj.SetValue(42);
                AssertEqual(obj.GetValue(), 42, "test set then get");
    ? Operation order. 
        Varying the sequence of operations to perform the same test (where this is possible) can help uncover any order of execution assumptions and non-orthogonal behavior, 
            that is, if API calls have undocumented side effects that are being relied upon to achieve certain workflows.
    ? Regression testing. 
        Backward compatibility with earlier versions of the API should be maintained whenever possible. 
        It is therefore extremely valuable to have tests that verify this goal.
            For example, a test could try reading data files that were generated by older versions of the API to ensure that the latest version can still ingest them correctly. 
                It's important that these data files are never updated to newer formats when the API is modified. 
                That is, you will end up with live data files, which are up to date for the current version, and legacy data files, which verify the backward compatibility of the API.
    ? Negative testing. 
        This testing technique constructs or forces error conditions to see how the code reacts to unexpected situations. 
            For example, if an API call attempts to read a file on disk, a negative test might try deleting that file, or making it unreadable, to see how the API reacts when it is unable to read the contents of the file. 
            Another example of negative testing is supplying invalid data for an API call. 
                For example, a credit card payment system that accepts credit card numbers should be testedwith invalid credit card numbers (negative testing) as well as valid numbers (positive testing).
    ? Buffer overruns. 
        A buffer overrun, or overflow, is when memory is written past the end of an allocated buffer. 
            This causes unallocated memory to be modified, often resulting in data corruption and ultimately a crash. 
            Data corruption errors can be difficult to track down because the crash may occur some time after the actual buffer overrun event. 
        It is therefore good practice to check that an API does not write to memory beyond the size of a buffer. 
        This buffer could be an internal private member of a class or it could be a parameter that you pass into an API call.
            For example, the string.h function strncpy() copies at most n characters from one string to another. 
            This could be tested by supplying source strings that are equal to and longer than n characters and then verifying that no more than n characters (including the null terminator, \ 0) are written to the destination buffer.
    ? Memory ownership. 
        Memory errors are a common cause of crashes in C++ programs. 
        Any API calls that return dynamically allocated memory should document whether the API owns the memory or if the client is responsible for freeing it. 
            These specifications should be tested to ensure that they are correct. 
                For example, if the client is responsible for freeing the memory, a test could request the dynamic object twice and assert that the two pointers are different. 
                A further test could free the memory and then rerequest the object from the API multiple times to ensure that no memory corruption or crashes occur.
    ? NULL input. 
        Another common source of crashes in C++ is passing a NULL pointer to a function that then immediately attempts to dereference the pointer without checking for NULL. 
        You should therefore test all functions that accept a pointer parameter to ensure that they behave gracefully when passed a NULL pointer.



10.3.3 Focusing the Testing Effort
    1. Focus on tests that exercise primary use cases or workflows of the API.
    2. Focus on tests that cover multiple features or offer the widest code coverage.
    3. Focus on the code that is the most complex and hence the highest risk.
    4. Focus on parts of the design that are poorly defined.
    5. Focus on features with the highest performance or security concerns.
    6. Focus on testing problems that would cause the worst impact on clients.
    7. Focus early testing efforts on features that can be completed early in the development cycle.
10.3.4 Working with QA
    it¡¯s standard practice for developers to write and own unit tests and for QA to write and own integration tests.

    providing script bindings for your API can offer greater opportunity for your QA team to contribute automated integration tests
    write programs that enable data-driven testing

10.4 WRITING TESTABLE CODE
    you should consider how a class will be tested early on during its development.

10.4.1 Test-Driven Development
    Test-Driven Development (TDD), or Test-First Programming, involves writing automated tests to verify desired functionality before the code that implements this functionality is written. 
        These tests will of course fail initially. 
        The goal is then to quickly write minimal code to make these tests pass.
        Then finally the code is refactored to optimize or clean up the implementation as necessary (Beck, 2002).
    An important aspect of TDD is that changes are made incrementally, in small steps. 
        You write a short test, then write enough code to make that test pass, and then repeat. 
        After every small change, you recompile your code and rerun the tests. 
        Working in these small steps means that if a test starts to fail, then in all probability this will be caused by the code you wrote since the last test run.

TIP
    Test Driven Development means that you write unit tests first and then write the code to make the tests pass. 
    This keeps you focused on the key use cases for your API.


TDD does not have to be confined to the initial development of your API. 
    It can also be helpful during maintenance mode. 
    For example, when a bug is discovered in your API, you should first write a test for the correct behavior. 
    This test will of course fail at first. You can then work on implementing the fix for the bug. 
    You will know when the bug is fixed because your test will change to a pass state. 
    Once the bug is fixed, you then have the added benefit of an ongoing regression test that will ensure that the same bug is not introduced again in the future.


10.4.2 Stub and Mock Objects
    One popular technique to make your unit tests more stable and resilient to failures is to create test objects that can stand in for real objects in the system. 
        This lets you substitute an unpredictable resource with a lightweight controllable replacement for the purpose of testing. 
            Examples of unpredictable resources include the file system, external databases, and networks. 
        The stand-in object can also be used to test error conditions that are difficult to simulate in the real system, as well as events that are triggered at a certain time or that are based on a random number generator.
? Fake object
    An object that has functional behavior but uses a simpler implementation to aid testing, 
        for example, an in-memory file system that simulates interactions with the local disk.
? Stub object
    An object that returns prepared or canned responses. 
        For example, a ReadFileAsString() stub might simply return a hardcoded string as the file contents rather than reading the contents of the named file on disk.
? Mock object
    An instrumented object that has a preprogrammed behavior and that performs ver-ification on the calling sequence of its methods. 
        For example, a mock object (or simply a mock) can specify that a GetValue() function will return 10 the first two times it¡¯s called and then 20 after that. 
    It can also verify that the function was called, say, only three times or at least five times or that the functions in the class were called in a given order.

    One of the main differences between mock and stub objects is that mocks insist on behavior verification. 
        That is, a mock object is instrumented to record all function calls for an object and it will verify behavior such as the number of times a function was called, the parameters that were passed to the function, or the order in which several functions were called. 
    Writing code to perform this instrumentation by hand can be tedious and error prone. 
        It is therefore best to rely upon a mock testing framework to automate this work for you. 
        Google Mock framework (http://code.google.com/p/googlemock/)


TIP
    Stub and mock objects can both return canned responses, but mock objects also perform call behavior verification.

    In terms of how this affects the design of your APIs, one implication is that you may wish to consider a model where access to unpredictable resources is embodied within a base class that you pass into your worker classes.
        This allows you to substitute a stub or mock version in your test code using inheritance. 
        This is essentially the dependency injection pattern, where dependent objects are passed into a class rather than that class being directly responsible for creating and storing those objects.
    
    However, sometimes it is simply not practical to encapsulate and pass in all of the external dependencies for a class. 
        In these cases, you can still use stub or mock objects, but instead of using inheritance to replace functionality, you can inject them physically at link time. 
        In this case, you name the stub/mock class the same as the class you wish to replace. 
        Then your test program links against the test code and not the code with the real implementation. 
    This approach can be very powerful, although it does necessitate that you create your own abstractions for accessing the file system, network, and so on. 
        If your code directly calls fopen() from the standard library, then you cant replace this with a stub at link time unless you also provide stubs for all other standard library functions that your code calls.


10.4.3 Testing Private Code
    1. Member function: Declaring a public MyClass::SelfTest() method.
        there are some undesirable qualities of this approach. 
            1) you have to pollute your public API with a method that your clients should not call
                there are ways that you can discourage clients from using this function. 
                One trivial way to do this would be to simply add a comment that the method is not for public use. 
                Taking this one step further, you could remove the method from any API documentation you produce so that users never see a reference to it (unless they look directly at your headers of course). 
                    You can achieve this with the Doxygen tool by surrounding the function declaration with the \cond and \endcond commands.
                    /// \cond TestFunctions
                    void SelfTest();
                    /// \endcond
            2) you may add extra bloat to your library by embedding the test code inside the implementation.
                - implement the SelfTest() method in your unit test code, not in the main API code
                    Just because you declare a method in your .h file doesnt mean that you have to define it. 
                    However, this opens up a similar security hole to using friends. 
                    That is, your clients could define the SelfTest() method in their own code as a way to modify the internal state of the object. 
                    While the interface of this function restricts what they can do, because they cannot pass in any arguments or receive any results, they can still use global variables to circumvent this.
                - conditionally compile the test code
                    void SelfTest()
                    {
                    #ifdef TEST
                        // lots of test code
                    #else
                        std::cout<<"Self test code not compiled in."<<std::endl;
                    #endif
                    }
                    
                    The downside of this approach is that you have to build two versions of your API: one with the self-test code compiled in (compiled with -DTEST or /DTEST) and one without the self-test code. 
                    If the extra build is a problem, you could compile the self-test code into debug versions of your library 
                    but remove it from release builds.
                    
    2. Friend function: Creating a MyClassSelfTest() free function and declaring it as friend function in MyClass.
        the friend function can be made relatively safe if the MyClassSelfTest() function is defined in the same library as the MyClass implementation, thus preventing clients from redefining the function in their own code. 
            Of note, the Google Test framework provides a FRIEND_TEST() macro to support this kind of friend function testing. 
        However, because the two options are functionally equivalent, and given our general preference to avoid friends unless absolutely necessary, 
            I will concentrate on the first of these options: adding a public SelfTest() method to a class to test its internal details



TIP
    Use a SelfTest() member function to test private members of a class.



10.4.4 Using Assertions
    An assertion is a way to verify assumptions that your code makes. 
        You do this by encoding the assumption in a call to an assert function or macro. 
        If the value of the expression evaluates to true, then all is well and nothing happens. 
        However, if the expression evaluates to false, then the assumption you made in the code is invalid and your program will abort with an appropriate error (McConnell, 2004).
    Assertions are essentially a way for you to include extra sanity tests for your program state directly in the code. 
        As such, these are invaluable complementary aids to help testing and debugging.
        Although you are free to write your own assertion routines, the C standard library includes an assert() macro in the assert.h header (also available in C++ as the cassert header).

    It is common practice to turn off all assert() calls for production code so that an end-user application doesn¡¯t abort needlessly when the user is running it. 
        This is often done by making assert calls do nothing when they are compiled in release mode versus debug mode. 
        (For this reason, you should never put code that must always be executed into an assertion.)
        // from standard: 
        // <cassert> or <assert.h> depends each time on the lexically current definition of NDEBUG.


TIP
    Use assertions to document and verify programming errors that should never occur.
    // "never" occur

    // compile time assertion (C++0x only)
    static_assert(sizeof(void *)==32, "This code only works on 32 bit platforms.");


TIP
    Enforce an interface¡¯s contract through the systematic use of assertions, 
        such as require(), ensure(), and check_invariants().
        
        #ifdef DEBUG
        // turn on contract checks in a debug build
        #define require(cond) assert(cond)
        #define ensure(cond) assert(cond)
        #define check_invariants(obj) assert(obj && obj->IsValid());
        #else
        // turn off contract checks in a non debug build
        #define require(cond)
        #define ensure(cond)
        #define check_invariants(obj)
        #endif


    Nevertheless, one of the benefits of employing this kind of contract programming is that errors get flagged much closer to the actual source of the problem. 
        This can make a huge difference when trying to debug a complex program, as very often the source of an error and the point where it causes a problem are far apart. 
        This is of course a general benefit of using assertions.


TIP
    Perform contract checks against the interface, not the implementation.
        That is, your precondition and postcondition checks should make sense at the abstraction level of your API. 
        They should not depend on the specifics of your particular implementation, as otherwise you will find that you have to change the contract whenever you change the implementation.


10.4.6 Record and Playback Functionality
    One feature that can be invaluable for testing (and many other tasks) is the ability to record the sequence of calls that are made to an API and then play them back again at will. 
        Record and playback tools are fairly common in the arena of application or GUI testing, where user interactions such as button presses and keystrokes are captured and then played back to repeat the users actions. 
    However, the same principles can be applied to API testing. 
        This involves instrumenting every function call in your API to be able to log its name, parameters, and return value. 
        Then a playback module can be written that accepts this log, or journal, file and calls each function in sequence, checking that the actual return values match the previously recorded responses.
    Ordinarily this functionality will be turned off by default so that the overhead of creating the journal file does not impact the performance of the API. 
        However, it can be switched on in a production environment to capture actual end-user activity. 
        These journal files can then be added to your test suite as data-driven integration tests or can be played back in a debugger to help isolate problems. 
        You can even use them to refine the behavior of your API based on real-world usage information, such as detecting common invalid inputs and adding better error handling for these cases. 
    Your clients could even expose this functionality in their applications to allow their end users to record their actions and play them back themselves, that is, to automate repetitive tasks in the application. 
        This is often called a macro capability in end-user applications.
    There are several different ways that you could instrument your API in this fashion. 
        One of the more clean ways to do this is to introduce a Proxy API that essentially forwards straight through to your main API, but which also manages all of the function call logging. 
            In this way, you dont need to pollute your actual API calls with these details and you always have the option of shipping a vanilla API without any logging functionality. 

            bool PlaySound(const std::string &filename)
            {
                LOG FUNCTION("PlaySound");
                LOG PARAMETER(filename);
                bool result detail::PlaySound(filename);
                LOG RESULT(result);
                return result;
            }



    if you already have a wrapper API, such as a script binding or a convenience API, 
        then you can simply reuse that interface layer. 
        This is also a good place to perform your API contract tests

    Gerard Meszaros notes that on its face, record and playback techniques may appear to be counter to agile methodologies such as test-first development. 
        However, he points out that it is possible to use record and playback in conjunction with test-first methodologies as long as the journal is stored in a human-readable file format such as XML (Meszaros, 2003). 
        When this is the case, the record and playback infrastructure can be built early on and then tests can be written as data files rather than in code. 
        This has the additional benefit that more junior QA engineers could also contribute data-driven integration tests to the test suite.

    Adding robust record and playback functionality to your API can be a significant undertaking,
        but the costs are normally worth it when you consider 
            the benefits of faster test automation 
            and the ability to let your clients easily capture reproduction cases for bug reports.



10.4.7 Supporting Internationalization
    Internationalization (i18n) is the process of enabling a software product to support different languages and regional variations. 
    The related term localization (l10n) refers to the activity of using the underlying internationalization support to provide translations of application text into a specific language and to define the locale settings for a specific region, such as the date format or currency symbol.

    design decisions made during the development of your API can have an impact on 
        how easily your clients can provide localization support in their applications.
    Several libraries provide internationalization and localization functionality. 
        You could use one of these libraries to return localized strings to your clients and let them specify the preferred locale for the strings that your API returns. 
        These libraries are often very easy to use. 
            the GNU gettext library provides a gettext() function to look up the translation for a string and return it in the language for the current locale (assuming that a translation has been provided). 
                Often, this gettext() function is aliased to _ so that you can write simple code such as
                    std::cout << _("Please enter your username:");
            the Qt library provides excellent internationalization and localization features. 
                All QObject subclasses that use the Q_OBJECT macro have a tr() member function that behaves similarly to GNUs gettext() function, for example,
                    button = new QPushButton(tr("Quit"), this);


10.5 AUTOMATED TESTING TOOLS
    four broad categories
        1. Test harnesses. 
            Software libraries and programs that make it easier to maintain, run, and report results for automated tests.
        2. Code coverage. 
            Tools that instrument your code to track the actual statements or branches that your tests executed.
        3. Bug tracking.
            A database-driven application that allows defect reports and feature requests to be submitted, prioritized, assigned, and resolved for your software.
        4. Continuous build systems. 
            A system that rebuilds your software and reruns your automated tests whenever a new change is added.


10.5.1 Test Harnesses
    There are many unit test frameworks available for C and C++. 
    Most of these follow a similar design to the classic JUnit framework and provide support for features such as assertion-based testing, fixture setup, grouping of fixtures for multiple tests, and mock objects. 
    In addition to being able to define a single test, a good test framework should also provide a way to run an entire suite of tests at once and report the total number of failures.

? CppUnit(http://cppunit.sourceforge.net/)
    A port of JUnit to C++ originally created by Michael Feathers. 
    This framework supports 
        various helper macros to 
            simplify the declaration of tests, 
            capturing exceptions, 
        and a range of output formats, including 
            an XML format 
            and a compiler-like output to ease integration with an IDE. 
    CppUnit also provides a number of different test runners, including 
        Qt- and MFC-based GUI runners. 
    Version 1 of CppUnit has reached a stable state and future development is being directed toward CppUnit 2. 
    Michael Feathers has also created an extremely lightweight alternative version of CppUnit called CppUnitLite.

    class ComplexNumberTest : public CppUnit::TestFixture
    {
    public:
        void setUp()
        {
            m_10_1 = new Complex(10, 1);
            m_1_1 = new Complex(1, 1);
            m_11_2 = new Complex(11, 2);
        }
        void tearDown()
        {
            delete m_10_1;
            delete m_1_1;
            delete m_11_2;
        }
        void testEquality()
        {
            CPPUNIT_ASSERT(*m_10_1 == *m_10_1);
            CPPUNIT_ASSERT(*m_10_1 != *m_11_2);
        }
        void testAddition()
        {
            CPPUNIT_ASSERT(*m_10_1 + *m_1_1 == *m_11_2);
        }
    private:
        Complex *m_10_1;
        Complex *m_1_1;
        Complex *m_11_2;
    };



? Boost Test(http://www.boost.org/)
    Boost includes a Test library for writing test programs, organizing tests into simple test cases and test suites, and controlling their run-time execution.
        A core value of this library is portability. 
            As such it uses a conservative subset of C++ features and minimizes dependencies on other APIs. 
            This has allowed the library to be used for porting and testing of other Boost libraries. 
    Boost Test provides an execution monitor that can catch exceptions in test code, as well as a program execution monitor that can check for exceptions and non-zero return codes from an end-user application. 
    The following example, derived from the Boost Test manual, demonstrates how to write a simple unit test using this library.
    #define BOOST_TEST_MODULE MyTest
    #include<boost/test/unit_test.hpp>
    int add(int i, int j)
    {
        return i+j;
    }
    BOOST_AUTO_TEST_CASE(my_test)
    {
        // #1 continues on error
        BOOST_CHECK(add(2, 2) == 4);
        // #2 throws an exception on error
        BOOST_REQUIRE(add(2, 2) == 4);
        // #3 continues on error
        if (add(2, 2) != 4)
            BOOST_ERROR("Ouch...");
        // #4 throws an exception on error
        if (add(2, 2) != 4)
            BOOST_FAIL("Ouch...");
        // #5 throws an exception on error
        if (add(2, 2) != 4)
            throw "Ouch.. .";
        // #6 continues on error
        BOOST_CHECK_MESSAGE(add(2,2) == 4, "add() result: "<<add(2, 2));
        // #7 continues on error
        BOOST_CHECK_EQUAL(add(2, 2), 4);
    }

? Google Test(http://code.google.com/p/googletest/)
    The Google C++ Testing Framework provides a JUnit-style unit test framework for C++. 
    It is a cross-platform system that supports automatic test discovery 
        (i.e., you don¡¯t have to enumerate all of the tests in your test suite manually) 
    and a rich set of assertions, including 
        fatal assertions (the ASSERT_* macros), 
        non-fatal assertions (the EXPECT_* macros), 
        and so-called death tests (checks that a program terminates expectedly).
    Google Test also provides various options for running tests and offers textual and XML report generation. 
    As mentioned earlier, Google also provides a mock object testing framework, Google Mock, which integrates well with Google Test. 

    #include<gtest/gtest.h>
    bool IsPrime(int n);
    TEST(IsPrimeTest, NegativeNumbers)
    {
        EXPECT_FALSE(IsPrime(-1));
        EXPECT_FALSE(IsPrime(-100));
        EXPECT_FALSE(IsPrime(INT_MIN));
    }
    TEST(IsPrimeTest, TrivialCases)
    {
        EXPECT_FALSE(IsPrime(0));
        EXPECT_FALSE(IsPrime(1));
        EXPECT_TRUE(IsPrime(2));
        EXPECT_TRUE(IsPrime(3));
    }
    TEST(IsPrimeTest, PositiveNumbers)
    {
        EXPECT_FALSE(IsPrime(4));
        EXPECT_TRUE(IsPrime(5));
        EXPECT_FALSE(IsPrime(9));
        EXPECT_TRUE(IsPrime(17));
    }
    int main(int argc, char** argv)
    {
        ::testing::InitGoogleTest(&argc, argv);
        return RUN_ALL_TESTS();
    }



? TUT(http://tut-framework.sourceforge.net/)
    The Template Unit Test (TUT) Framework is a small portable C++unit test framework. 
    Because it consists only of header files, there is no library to link against or deploy. 
    Tests are organized into named test groups, and the framework supports automatic discovery of all tests that you define. 
    A number of test reporters are provided, including basic console output and a CppUnit-style reporter. 
        Its also possible to write your own reporters using TUTs extensible reporter interface. 
    Here is a simple canonical unit test written using the TUT framework.

    #include<tut/tut.hpp>
    namespace
    {
        tut::factory tf("basic test");
    }
    namespace tut
    {
        struct basic{};
        typedef test group<basic>factory;
        typedef factory::object object;
        
        template<>template<>
        void object::test<1>()
        {
            ensure_equals("2+2", 2+2, 4);
        }
        
        template<>template<>
        void object::test<2>()
        {
            ensure_equals("2*2", 2*2, 4);
        }
    }




10.5.2 Code Coverage
    Code coverage tools let you discover precisely which statements of your code are exercised by your tests, that is, these tools can be used to focus your testing activities on the parts of your code base that are not already covered by tests.
    Different degrees of code coverage can be measured.
    void TestFunction(int a, int b)
    {
        if (a == 1) a++; // Line 1
        int c = a * b; // Line 2
        if (a > 10 && b != 0) // Line 3
            c *= 2; // Line 4
        return a * c; // Line 5
    }
    ? Function coverage
        In this coarsest level of code coverage, only function calls are tracked. 
            In the example code, function coverage will only record whether TestFunction() was called at least once. 
            The flow of control within a function has no effect on function code coverage results.
    ? Line coverage
        This form of code coverage tests whether each line of code that contains an executable statement was reached. 
        One limitation of this metric can be seen on Line 1 of our code example. 
            Line coverage will consider Line 1 to be 100% exercised even if the a++ statement is not executed; it only matters if the flow of control hits this line. 
            Obviously, you can get a round this limitation by putting the if condition and the a++ statement on separate lines.
    ? Statement coverage
        This metric measures whether the flow of control reached every executable statement at least once. 
        The primary limitation of this form of coverage is that it does not consider the different code paths that can result from the expressions in control structures such as if, for, while, or switch statements. 
            For example, in our code sample, statement coverage will tell us if the condition on Line 3 evaluated to true, causing Line 4 to be executed. 
            However, it will not tell us if that condition evaluated to false because there is no executable code associated with that result.
    ? Basic block coverage ==>> statement coverage
        A basic block is a sequence of statements that cannot be branched into or out of. 
            That is, if the first statement is executed then all of the remaining statements in the block will also be executed. 
                Essentially, a basic block ends on a branch, function call, throw, or return.
                This can be thought of as a special case of statement coverage, with the same benefits and limitations.
    ? Decision coverage == branch coverage
        This code coverage metric measures whether the overall result of the expression in each control structure evaluated to both true and false. 
            This addresses the major deficiency of statement coverage because you will know if the condition in Line 3 evaluated to false. 
    ? Condition coverage
        Condition coverage determines whether each boolean subexpression in a control structure has evaluated to both true and false. 
            In our example here, this means that Line 3 must be hit with a>10, a<=10, b!=0, and b==0. 
        Note that this does not necessarily imply decision coverage, as each of these events could occur in such an order that the overall result of the if statement always evaluates to false.

Various programs let you measure the code coverage of C++ code. 
    Each of these supports different combinations of the metrics, normally by instrumenting the code that your compiler generates. 
    Most of these tools are commercial offerings, although there are some free and open source options too.

    exclude certain lines
        One feature in particular that can be very useful is the ability to exclude certain lines of code from the analysis, often done by adding special comments around those lines of code. 
            This can be used to turn off coverage for lines that legitimately can never be hit, 
                such as methods in a base class that are always overridden; 
            although in these cases its important that the coverage tool raises an error if the excluded code is ever hit in the future as this may signal an unexpected change in behavior.
    compiled without optimizations
        Another issue to bear in mind is that you should normally perform code coverage analysis on a build that has been compiled without optimizations, as compilers can reorder or eliminate individual lines of code during optimization.

    ? Gcov(http://gcc.gnu.org/onlinedocs/gcc/Gcov.html)
        This test coverage program is part of the open-source GNU GCC compiler collection. 
            It operates on code generated by g++ using the -fprofile-arcs and -ftest-coverage options. 
        Gcov provides function, line, and branch code coverage. 
        It outputs its report in a textual format; 
            however, the accompanying lcov script can be used to output results as an HTML report.

From experience, a high and very respectable degree of code coverage would be 100% function, 90% line, or 75% condition coverage.

code coverage and legacy code
    It's also worth specifically addressing the issue of code coverage and legacy code. 
        Sometimes your API must depend on a large amount of old code that has no test coverage at all. 
            Recall that Michael Feathers defines legacy code as code without tests (Feathers, 2004). 
    In these cases, it may be impractical to enforce the same code coverage targets for the legacy code that you impose for new code. 
    However, you can at least put some basic tests in place and then enforce that no checkin should lower the current coverage level. 
        This effectively means that any new changes to the legacy code should be accompanied with tests. 
    Because enforcing this requirement on a per checkin basis can sometimes be difficult 
        (because you have to build the software with the change and run all tests to know whether it should be accepted), 
      another reasonable way to make this work is to record the legacy code coverage for the previous version of the library 
        and ensure that coverage for the new version equals or exceeds this threshold at the time of release. 
        This approach offers a pragmatic way to deal with legacy code and lets you gradually increase the code coverage to acceptable levels over time.









10.5.3 Bug Tracking
A bug tracking system is an application that lets you keep track of bugs (and often suggestions) for your software project in a database.

Most bug tracking systems support the triage of incoming bugs, that is, setting the priority (and perhaps severity) of a bug and assigning it to a particular developer. 
    It¡¯s also standard practice to be able to define filters for the bugs in the system so that targeted bug lists can be created, 
        such as a list of open crashing bugs 
        or a list of bugs assigned to a particular developer. 
    Related to this, some bug tracking systems will also provide report generation functions, often with the ability to display graphs and pie charts. 

what a bug tracking system is not. 
    not a trouble ticket or issue tracking system. 
        These are customer support systems that are used to receive feedback fromusers, many of which may not be related to software problems. 
        Valid software issues that are discovered by customer support will then be entered into the bug tracking system and assigned to an engineer to work on.
    not a task or project management tool, 
        that is, a system that lets you track tasks and plan work. 
        However, some bug tracking systemvendors do provide complementary products that use the underlying infrastructure to provide a project management tool as well.

bug tracking systems
    Bugzilla (http://www.bugzilla.org/)
        many large open source projects use Bugzilla, including 
            Mozilla, Gnome, Apache, and the Linux Kernel.
    
    Google Code Hosting (http://code.google.com/hosting/)
        project hosting solution that provides 
            revision control features, disk storage quotas, discussion forums, and an integrated bug tracking system.
    



10.5.4 Continuous Build System
    A continuous build system is an automated process that rebuilds your software as soon as a new change is checked into your revision control system. 
    A continuous build system should be one of the first pieces of technology you put in place for a large project with multiple engineers, independent of any testing needs. 
        It lets you know the current state of the build and identifies build failures as soon as they happen. 
        It is also invaluable for cross-platform development because even the most well-tested change for one platform can break the build for a different platform. 

continuous build options
    ? Tinderbox(https://developer.mozilla.org/en/Tinderbox)
        An open source solution from the Mozilla team that runs builds and test suites. 
    ? TeamCity(http://www.jetbrains.com/teamcity/)
        A distributed build management and continuous integration server from JetBrains.
    ? Electric Cloud(http://www.electric-cloud.com/)
        A suite of tools for build/test/deploy automation that supports build parallelization.

the result of the automated build can be in one of four states: 
    build in progress, successful build, build failure, or test failure



It¡¯s important that you receive fast turnaround on builds so that if your test run starts taking several hours to complete, 
    then you should investigate some test optimization efforts. 
    
    One way to do this is to segment your tests into different categories and only run the fast category of tests as part of the continuous build.
    Another solution is to have multiple automated builds: 
        one that only builds the software 
        and another that builds the software and then runs all tests. 
    
        This gives engineers the ability to quickly receive feedback about build breakages while still ensuring that a full test suite is run as often as possible.









