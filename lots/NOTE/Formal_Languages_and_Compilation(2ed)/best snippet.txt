Formal_Languages_and_Compilation(2ed)::

3.5.1 Motivation of Nondeterminism
    The mapping from grammar rules to transitions pushed us to introduce some transitions with multiple destinations and spontaneous moves, which are the main forms of indeterminism. Since this might appear as a useless theoretical concept, we hasten to list several advantages.
    
    3.5.1.1 Concision
        Defining a language with a nondeterministic machine often results in a more readable and compact definition, as in the next example.
            Example 3.10(Penultimate character) ...
        // let L[k] be a language of sentences with b in the position -k, where ¦²={a,b} 
        To strengthen the argument consider a generalization of language L_2 to language L_k, such that, for some ke2, thekth character before the last is b. With little thought we see the nondeterministic automaton would have k+1 states, while one could prove that the number of states of the minimal deterministic machine is an exponential function of k.
        In conclusion nondeterminism sometimes allows much shorter definitions.
    
    3.5.1.2 Left Right Interchange and Language Reflection
        Nondeterminism also arises in string reversal when a given deterministic machine is transformed to recognize the reflection L^R of languageL. This is sometimes required when for some reason strings must be scanned from right to left.
        The new machine is straightforward to derive: interchange initial and final states and reverse all the arrows. Clearly this may give birth to nondeterministic transitions.
            Example 3.11 ...
    
    As a further reason, we anticipate that nondeterministic machines are the intermediate product of procedures for converting r.e. to automata, widely used for designing lexical analyzers or scanners.



4.11.1 Floyd¡¯s Operator-Precedence Grammars and Parsers
    Precedence Relations
    To grasp the intended meaning of the relations, it helps to consider the language of the arithmetic expressions with the operators plus ¡°+¡± (addition) and times ¡°¡Á¡± (multiplication). Since traditionally times takes precedence over plus, these two relations hold: ¡Á .>. + and + .<. ¡Á. The former relation means that in an expression such as 5¡Á3+7, the first reduction to apply is 5¡Á3 -> ¡¤¡¤¡¤. Similarly the fact that plus yields precedence to times (latter relation), says that in an expression such as 5+3¡Á7, the first reduction to apply is 3¡Á7 -> ¡¤¡¤¡¤.
    We are going to extend this traditional sense of precedence from the arithmetic operators to any terminal characters. Moreover we introduce the ¡°equal in precedence¡± relation .=. , to mean that the two related characters are adjacent in a grammar rule: thus the left and the right parentheses are related by ¡°(¡±.=.¡°)¡±in the rule E¡ú(F) for parenthesized arithmetic expressions.
    A word of warning: none of the relations enjoys the mathematical properties of reflexivity, symmetry, and transitivity, of the numerical order relations <, > and =, notwithstanding the similarity of the signs.

    Fig. 4.55 Precedence relations made apparent in the derivations of Example 4.88

    operator-precedence matrix (OPM)
    operator-precedence property <==> OPM conflict-free
    
    4.11.1.1 Comparison with Deterministic Languages
        We present a few examples to compare the languages of operator-precedence grammars and those of LL(1) and LR(1) grammars.
        A grammar that is deterministic but not OP, and that generates the language {a^n b a^n |n¡Ý0},is 
            S¡úaSa|b with matrix 
                         a          b
                a [ .<. .=. .>.    .<. ]
                b [     .>.            ]
        There are conflicts in the matrix entry[a, a], yet the grammar meets the LL(1) and therefore also the LR(1) condition.
        
        
        [symmetry with respect to mirror reversal]
        The OP grammars have the peculiar property of symmetry with respect to mirror reversal, in the sense that the grammar obtained by reversing the right parts of the rules, is also an OP grammar. Its precedence matrix simply interchanges the ¡°yield¡± and ¡°take¡± relations of the original grammar, and reverses the ¡°equal in precedence¡± relations. 
        To illustrate we list another grammar G:
            G: S¡úXb; X¡úaXb|ab; with OPM:
                     a      b
                a [ .<.    .=. ]
                b [        .>. ]
            
        The reversed language(L(G))^R is generated by the OP grammar G^R:
            G^R: S¡úbX; X¡úbXa|ba; with OPM:
                     a      b
                a [ .>.        ]
                b [ .=.    .<. ]

        On the other hand, in general neither a LL(1) nor a LR(1) grammar remains such when is reversed. For instance grammar G has the LL(1) property, but the reversed grammar G^R does not. The case of a language L that ceases to be deterministic upon reversal is illustrated by
            L={o a^n b^n |n¡Ý1} ¡È {t a^2n b^n |n¡Ý1}
        Here the first character (o or t) determines whether the string contains one a per b or two a¡¯s per b. This property is lost in the reversed language: L^R contains sentences of two types, say,b^4 a^4 o and b^4 a^8 t. A deterministic pushdown machine cannot decide when to pop a letter b upon reading an a.
        
        Property 4.91(Comparison of OP languages)
            ? The family of languages defined by Floyd OP grammars is strictly included in the family of deterministic (LR(1)) languages.
            ? Every OP grammar has the property that the mirror reversed grammar is also an OP grammar (so the mirror language remains deterministic).
            ? The families of languages defined by LL(1) and by OP grammars are not comparable.
        In practice OP grammars can be used to define quite a few existing technical languages, if their grammar has been converted into operator form. But there are languages where the use of the same terminal character in different constructs causes precedence conflicts, which may or may not be removed by suitable grammar transformation.


4.11.2 Sequential Operator-Precedence Parser
    However, a minor difficulty remains, if the grammar includes two rules such as A¡úx and B¡úx with a repeated right part: how to choose between nonterminals A and B when string x has been selected for reduction. This uncertainty could be propagated until just one choice remains, but to simplify matters, we assume that the grammar does not have any repeated right parts, i.e., it is invertible, a property that can be always satisfied by the transformation presented in Chap.2,p.59.


5.6.8.3 Semantics-Directed Parsing
    ... // keywords: guide predicate, multi-sweep attribute, L condition
    Focusing on artificial rather than natural languages, a reasonable assumption is that no sentence is semantically ambiguous, i.e., that every valid sentence has a unique meaning. Of course this does not exclude a sentence from being syntactically ambiguous. But then the uncertainty between different syntax trees can be solved at parsing time, by collecting and using semantic information as soon as it is available.
    For top-down parsing, we recall that the critical decision is the choice between alternative productions, when their ELL(1) guide sets (p.227) overlap on the current input character. Now we propose to help the parser solve the dilemma by testing a semantic attribute termed a guide predicate, which supplements the insufficient syntactic information. Such a predicate has to be computed by the parser, which is enhanced with the capability to evaluate the relevant attributes.
    Notice this organization resembles the multi-sweep attribute evaluation method described on p.351. The whole set of semantic attributes is divided in two parts assigned for evaluation to cascaded phases. The first set includes the guide predicates and the attributes they depend on; this set must be evaluated in the first phase, during parsing. The remaining attributes may be evaluated in the second phase, after the, by now unique, syntax tree has been passed to the phase two evaluator.
    We recall the requirements for the first set of attributes to be computable at parsing time: the attributes must satisfy the L condition (p.354). Consequently the guide predicate will be available when it is needed for selecting one of the alternative productions, for expanding a nonterminal Di in the production D0¡úD1...Di...Dr , with 1¡Üi¡Ür. Since the parser works depth-first from left to right, the part of the syntax tree from the root down to the subtrees D1...D[i-1] is then available.
    Following condition L, the guide predicate may only depend on the right attributes of D0 and on other (left or right) attributes of any symbol, which in the right part of the production precedes the root of the subtree Di under construction.
    The next example (Example5.52) illustrates the use of guide predicates.






5.7.2 Liveness Intervals of Variables
    ...
    To grasp the purpose of this definition, imagine that instruction p is the assignment a:=b¨’c, and suppose we want to know if some instruction makes use of the value assigned to the variable a in p. The question can be rephrased as: is variable alive out of node p? If not, the assignment p is useless and can be deleted without affecting the program semantics. Furthermore, if none of the variables used by p is used in some other instruction, all the instructions assigning a value to such variables may become useless after deleting instruction p. The next example (Example 5.55) illustrates the situation.



5.7.3 ReachingDefinitions
    ... // [constant propagation]
    Another basic and widely applied type of static analysis is the search for the variable definitions that reach some program point.
    To introduce the idea by an application, consider an instruction that assigns a constant value to variable a. The compiler examines the program to see if the same constant can be replaced for the variable in the instructions using a. The benefit of the replacement is manyfold. First, a machine instruction having a constant as operand (a so-called ¡°immediate¡± operand) is often faster. Second, substituting with a constant a variable occurring in an expression, may produce an expression where all the operands are constant. Then the expression value can be computed at compile time, with no need to generate any machine code for it. Lastly, since the replacement eliminates one or more uses of a, it shortens the liveness intervals and reduces the interferences between variables; thus the pressure on the processor registers is consequently reduced, too.
    The above transformation is termed constant propagation. In order to develop it, we need a few conceptual definitions, which are also useful for other program optimizations and verifications.





















